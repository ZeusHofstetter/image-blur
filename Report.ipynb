{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Automatized Picture Quality Assesment through Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the invention of digital photography the process of taking and processing holiday pictures has drastically changed. With analog photography the number of pictures per film role was limited to around 30, leading to only 100-200 pictures per holiday, and consequently each picture was taken with great care and precission.\n",
    "Now, the memory of a digital camera can hold many thousands of images and the quality of the pictures can be evaluated at a later point. This has lead to a steep increase of the number of pictures per holiday but also to a decrease in the average picture quality, as less care has to be taken during the photography process.\n",
    "\n",
    "Through this simultaneous increase in picture quantity together with a decrease in average picture quality the post-processing steps in the photography process have become much more important. For amateur photographs, such as us, this is illustrated through the Xmas-calendar problem. \n",
    "The Xmas-calendar problem typically manifests in the last weeks befor Christmas, when the thousands of holiday pictures of the last year have to be assesed. In a first step the pictures have to be sorted in usable and unusable images, where unusable images are mainly classified as being blurry, out of focuss or obscured by an unwanted object, such as the typical finger in front of the lense. In a second more demanding step the usable images have to be sorted according to photographic quality, in order to find to 12-20 best pictures.\n",
    "\n",
    "In this project we develop and test a machine learning (ML) method in order to automatize these two picture quality assesment tasks. In a first step we use the CERTH Image Blur Dataset [1] to develope and test the performance of different ML methods to classify blurred and out of focuss images. Next, the top XX performing ML methods are tested on a personal dataset of blurred and non-blurred pictures, to determine the transferability of the chosen approach. If the chosen ML approach is indeed transferable, we will extend the CERTH Image Blur Dataset [1] with the personal dataset of blurred and non-blurred pictures, as well as a personal data set of obstructed images. We will then train and test the ML methods on this extended data-set to correctly classify the pictures as usable or unusable. \n",
    "\n",
    "In an extention to this work, or if we finish the first part soon enoughf, we propose to extend the ML approach, either by retraining the first ML classifieror by training a second ML classifier, to also sort the usable images based on photography quality. \n",
    "\n",
    "[1] E. Mavridaki, V. Mezaris, \"No-Reference blur assessment in natural images using Fourier transform and spatial pyramids\", Proc. IEEE International Conference on Image Processing (ICIP 2014), Paris, France, October 2014. http://mklab.iti.gr/project/imageblur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection of ML approach based on the CERTH Image Blur Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline for classification accuracy\n",
    "\n",
    "First we establish a baseline to compare our more advanced models agains. \n",
    "The testing and training of the baseline can be found in the file transferVGG16.ipynb.\n",
    "The classification accuracy is calculated as:\n",
    "\n",
    "##### Prediction accuracy of Dummy Classifier\n",
    "Train: 0.588 <br>\n",
    "Test: 0.554\n",
    "\n",
    "##### Prediction accuracy of Random Forest Classifier\n",
    "Train: 0.999 <br>\n",
    "Test: 0.600\n",
    "\n",
    "##### Prediction accuracy for VGG16 with Dummy Classifier\n",
    "Train: 0.605 <br>\n",
    "Test: 0.543\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning based on VGG16 with logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validation for different models and model parameters of the transfer learning based on VGG16 can be found in the file transferVGG16.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Prediction accuracy for chosen VGG16 model with Logistic Regression\n",
    "training score:  1.0 <br>\n",
    "test score:  0.727"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning based on other pre-trained models together with logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the testing with VGG16 it seems that extracting the output at the last pooling layer is the most promising.\n",
    "Now we are going to test the performance of some other pre-trained Neural Networks. The full script can be found in transfer_lerning_v2.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Prediction accuracy for VGG19 model with Logistic Regression\n",
    "training score:  1.0 <br>\n",
    "test score:  0.712\n",
    "\n",
    "##### Prediction accuracy for ResNet50 model with Logistic Regression\n",
    "training score:  1.0 <br>\n",
    "test score:  0.784\n",
    "\n",
    "##### Prediction accuracy for DenseNet121 model with Logistic Regression\n",
    "training score:  1.0 <br>\n",
    "test score:  0.714\n",
    "\n",
    "##### Prediction accuracy for Xception model with Logistic Regression\n",
    "training score:  1.0 <br>\n",
    "test score:  0.653"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of all the transfer learning models the ResNet50 has the highest test score. But even for the ResNet50 model the test score is still quite low. This might either be due to the poor performance of the Logistic Regression Classifier or due to the fact that most image recognition Neural Networks include some kind of convolutional blurring, which will probably negatively impact the classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning based on ResNet50 with different classifiers\n",
    "\n",
    "Here the idea is to see if we can improve the prediction accuracy, by using different Classifiers instead of Logistic Regression in combination with transfer learning from ResNet50. The full script can be found in ResNet50.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Prediction accuracy for ResNet50 model with Random Forest Classifier\n",
    "training score:  0.9988235294117647 <br>\n",
    "test score:  0.753\n",
    "\n",
    "##### Prediction accuracy for ResNet50 model with Extra Trees Classifier\n",
    "training score:  1.0 <br>\n",
    "test score:  0.736\n",
    "\n",
    "##### Prediction accuracy for ResNet50 model with Gradient Boosting Classifier\n",
    "training score:  1.0 <br>\n",
    "test score:  0.775\n",
    "\n",
    "##### Prediction accuracy for ResNet50 model with KNeighboors Classifier\n",
    "training score:  0.8411764705882353 <br>\n",
    "test score:  0.668"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the transfer learning models is still not satisfactory. We still have to test how custom Neural Networks without convolutional bluring layers will perform. In a next step we also test how the different 'shallow learning' classifiers perform without the initial transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  'Shallow learning' models without Neural-Network based deep learning\n",
    "\n",
    "The main idea here is to see how well shallow models without deep learning will perform.\n",
    "The main script can be found in shallow_learning_models.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Prediction accuracy for Random Forest Classifier\n",
    "training score:  0.9929411764705882 <br>\n",
    "test score:  0.597\n",
    "\n",
    "##### Prediction accuracy for Extra Trees Classifier\n",
    "training score:  0.9964705882352941 <br>\n",
    "test score:  0.604\n",
    "\n",
    "##### Prediction accuracy for Gradient Boosting Classifier\n",
    "training score:  1.0 <br>\n",
    "test score:  0.614\n",
    "\n",
    "##### Prediction accuracy for KNeighboors Classifier\n",
    "training score:  0.7858823529411765 <br>\n",
    "test score:  0.61\n",
    "\n",
    "##### Prediction accuracy for Logistic Regression Classifier\n",
    "training score:  0.9788235294117648 <br>\n",
    "test score:  0.577"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that by using the transfer learning with ResNet50 we significantly increase the classification accuracy (from ~0.6 up to ~0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We tried to delevop and optimize a our own neural network instead of using an existing one. The script with all the custom neural network details and optimization can be found in customNN.ipynb\n",
    "\n",
    "We decided to use two approaches. First, we decide to create a NN able to edge recognition, which we believe is a good feature in order to distinguish blurred images (we made two different NN, one with only one dense layer after convolution used to classify blurried images, and a second one with 3 dense layers with 30 nodes before the last classification). Then, we created a NN without any specification during convolution, we optimized the number of layers and node during convolution and the number of dense layers in order to obtain the best prediction. The results are the following:\n",
    "\n",
    "##### NN with edge recognition\n",
    "training score:\t0.976 <br>\n",
    "test score: 0.684\n",
    "\n",
    "##### NN with edge recognition and 3 dense layers\n",
    "training score:\t0.995 <br>\n",
    "test score: 0.681\n",
    "\n",
    "##### NN with normal convolution\n",
    "training score: 0.993 <br>\n",
    "test score: 0.77\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personal blurred dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of the personal dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The personal image-blur data-set can be found as DATA/Personal_ImageBlurDataset.tar. The classification for the validation was done manually using the notebook personal_database_generation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing of the different ML approaches\n",
    "To test if the trained models are directly transferable to other data-sets (taken with other cameras etc.), we evaluate the accuracy of the best performing transfer learning model (ResNet50 plus a Logistic Regression Classifier, a Gradient Boosting Classifier or a Random Forest Classifier) and of the best performing custom Neural Network on the personal dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Transfer Learning for personal dataset\n",
    "The script for the prediction using ResNet50 plus a Logistic Regression Classifier can be found at personal_transfer.ipynb.\n",
    "\n",
    "###### LogisticRegression stats\n",
    "training score:  1.0 <br>\n",
    "test score:  0.9186046511627907\n",
    "\n",
    "###### GradientBoostingClassifier stats\n",
    "training score:  1.0 <br>\n",
    "test score:  0.9302325581395349\n",
    "\n",
    "###### RandomForrestClassifier stats\n",
    "training score:  0.9988235294117647 <br>\n",
    "test score:  0.8488372093023255\n",
    "\n",
    "###### Custom NN with edge recognition\n",
    "training score: 0.999   <br>\n",
    "test score: 0.674\n",
    "\n",
    "###### Custom NN\n",
    "training score:  0.993 <br>\n",
    "test score: 0.86\n",
    "\n",
    "For the personal image-blur dataset the Gradient Boosting Classifier reaches 93% accuracy. This in an improvement of around 15% compared to the accuracy for the CERTH test set. This result indicates two points. <br>\n",
    "a) The CERTH test set is much harder to classify, which is due to the fact that in this set even very small blurred areas included, while the personal test set only includes either undistorted or strongly blurred images. <br>\n",
    "b) The models trained on the CERTH set are directly transferable to our personal test sets and can thus be combined or extended with personal data. This opens the path to extend the CERTH set with obscured images, in order to then distinguish between three classes: blurred, undistorted and obscured. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
